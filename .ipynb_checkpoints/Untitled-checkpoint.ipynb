{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c46d723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading model...\n",
      "‚ùå Error loading model: Object of type ellipsis is not JSON serializable\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type ellipsis is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6164\\2158668150.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"üîÑ Loading model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"‚úÖ Model loaded successfully!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\saving\\saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    194\u001b[0m         )\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".h5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\".hdf5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0msaving_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras_option_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_legacy_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             model = saving_utils.model_from_config(\n\u001b[0m\u001b[0;32m    134\u001b[0m                 \u001b[0mmodel_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# Replace keras refs with keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_find_replace_nested_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"keras.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"keras.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return serialization.deserialize_keras_object(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\u001b[0m in \u001b[0;36m_find_replace_nested_dict\u001b[1;34m(config, find, replace)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_find_replace_nested_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[0mdict_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[0mdict_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[1;32mE:\\anaconda\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[0;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type ellipsis is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# app.py\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from werkzeug.utils import secure_filename\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Configuration\n",
    "UPLOAD_FOLDER = 'uploads'\n",
    "ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg'}\n",
    "MODEL_PATH = 'saved_models/resnet50_skin_cancer.h5'\n",
    "METRICS_PATH = 'saved_models/resnet50_metrics.json'\n",
    "CLASS_INDICES_PATH = 'saved_models/class_indices.json'\n",
    "\n",
    "# Create upload folder\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
    "\n",
    "# Load model and metrics\n",
    "try:\n",
    "    print(\"üîÑ Loading model and metrics...\")\n",
    "    model = load_model(MODEL_PATH)\n",
    "    with open(METRICS_PATH) as f:\n",
    "        metrics = json.load(f)\n",
    "    with open(CLASS_INDICES_PATH) as f:\n",
    "        class_indices = json.load(f)\n",
    "        class_labels = {v: k for k, v in class_indices.items()}\n",
    "    print(\"‚úÖ Model and metrics loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model or metrics: {e}\")\n",
    "    raise\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html', \n",
    "                          accuracy=f\"{metrics['accuracy']*100:.2f}%\",\n",
    "                          precision=f\"{metrics['precision']*100:.2f}%\",\n",
    "                          recall=f\"{metrics['recall']*100:.2f}%\",\n",
    "                          f1_score=f\"{metrics['f1_score']*100:.2f}%\")\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({'error': 'No file part'})\n",
    "\n",
    "    file = request.files['file']\n",
    "    if file.filename == '':\n",
    "        return jsonify({'error': 'No selected file'})\n",
    "    \n",
    "    if not allowed_file(file.filename):\n",
    "        return jsonify({'error': 'File type not allowed'})\n",
    "\n",
    "    filename = secure_filename(file.filename)\n",
    "    filepath = os.path.join(UPLOAD_FOLDER, filename)\n",
    "    file.save(filepath)\n",
    "\n",
    "    try:\n",
    "        # Preprocess image\n",
    "        img = image.load_img(filepath, target_size=(224, 224))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = model.predict(img_array)\n",
    "        predicted_class = class_labels[int(prediction[0][0] > 0.5]\n",
    "        confidence = float(prediction[0][0] if predicted_class == 'malignant' else 1 - prediction[0][0])\n",
    "        \n",
    "        return jsonify({\n",
    "            'prediction': predicted_class,\n",
    "            'confidence': f\"{confidence*100:.2f}%\",\n",
    "            'image_url': f\"/uploads/{filename}\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)})\n",
    "    finally:\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "\n",
    "@app.route('/metrics')\n",
    "def get_metrics():\n",
    "    return jsonify(metrics)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01efedb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (297043492.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_10864\\297043492.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    ```python\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# FastAPI for backend\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "df = data.frame\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "# ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# ŸÖŸÇŸäÿßÿ≥ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ÿßÿÆÿ™Ÿäÿßÿ± ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨\n",
    "model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)\n",
    "\n",
    "# ÿßŸÑÿ™ÿØÿ±Ÿäÿ®\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ÿßŸÑÿ™ŸÜÿ®ÿ§\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# ÿ™ŸÇŸäŸäŸÖ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"‚úÖ Accuracy:\", acc)\n",
    "print(\"\\nüìã Classification Report:\\n\", classification_report(y_test, y_pred, target_names=data.target_names))\n",
    "\n",
    "# ÿ•ŸÜÿ¥ÿßÿ° ŸÖÿ¨ŸÑÿØ static ÿ•ÿ∞ÿß ŸÑŸÖ ŸäŸÉŸÜ ŸÖŸàÿ¨ŸàÿØŸãÿß\n",
    "STATIC_DIR = Path(\"static\")\n",
    "STATIC_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ŸÖÿµŸÅŸàŸÅÿ© ÿßŸÑÿßŸÑÿ™ÿ®ÿßÿ≥\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=data.target_names, yticklabels=data.target_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(STATIC_DIR / \"confusion_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# ÿ≠ŸÅÿ∏ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸàÿßŸÑŸÖŸÇŸäÿßÿ≥\n",
    "MODEL_DIR = Path(\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "with open(MODEL_DIR / \"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open(MODEL_DIR / \"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# ÿ≠ŸÅÿ∏ ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÖŸäÿ≤ÿßÿ™\n",
    "feature_info = {\n",
    "    \"feature_names\": data.feature_names.tolist(),\n",
    "    \"target_names\": data.target_names.tolist()\n",
    "}\n",
    "\n",
    "with open(MODEL_DIR / \"feature_info.json\", \"w\") as f:\n",
    "    json.dump(feature_info, f)\n",
    "\n",
    "# ÿ•ŸÜÿ¥ÿßÿ° ÿ™ÿ∑ÿ®ŸäŸÇ FastAPI\n",
    "app = FastAPI(title=\"Breast Cancer Classifier\")\n",
    "\n",
    "# Mount static files\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "\n",
    "class PredictionInput(BaseModel):\n",
    "    features: list[float]\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(input_data: PredictionInput):\n",
    "    try:\n",
    "        # ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸàÿßŸÑŸÖŸÇŸäÿßÿ≥\n",
    "        with open(MODEL_DIR / \"model.pkl\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        with open(MODEL_DIR / \"scaler.pkl\", \"rb\") as f:\n",
    "            scaler = pickle.load(f)\n",
    "        \n",
    "        # ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑŸÖÿØÿÆŸÑÿßÿ™ ÿ•ŸÑŸâ ŸÖÿµŸÅŸàŸÅÿ© numpy\n",
    "        features = np.array(input_data.features).reshape(1, -1)\n",
    "        \n",
    "        # ÿ™ÿ∑ÿ®ŸäŸÇ ÿßŸÑŸÖŸÇŸäÿßÿ≥\n",
    "        scaled_features = scaler.transform(features)\n",
    "        \n",
    "        # ÿπŸÖŸÑ ÿßŸÑÿ™ŸÜÿ®ÿ§\n",
    "        prediction = model.predict(scaled_features)\n",
    "        probability = model.predict_proba(scaled_features).max()\n",
    "        \n",
    "        # ÿ™ÿ≠ŸÖŸäŸÑ ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÖŸäÿ≤ÿßÿ™\n",
    "        with open(MODEL_DIR / \"feature_info.json\", \"r\") as f:\n",
    "            feature_info = json.load(f)\n",
    "        \n",
    "        return {\n",
    "            \"prediction\": int(prediction[0]),\n",
    "            \"probability\": float(probability),\n",
    "            \"class_name\": feature_info[\"target_names\"][prediction[0]],\n",
    "            \"feature_names\": feature_info[\"feature_names\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "@app.get(\"/model-info\")\n",
    "async def get_model_info():\n",
    "    with open(MODEL_DIR / \"feature_info.json\", \"r\") as f:\n",
    "        feature_info = json.load(f)\n",
    "    return feature_info\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52fd26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
